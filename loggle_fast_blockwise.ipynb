{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rohit\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\rohit\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.linalg import eigh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_varying_precision(p=10, N=50, eps=1e-6, soft_lambda=0.14):\n",
    "    \"\"\"\n",
    "    Generate a list of time-varying precision matrices Omega(t),\n",
    "    following the four steps (i)-(iv) described:\n",
    "\n",
    "      (i)  B_i are lower-triangular with entries ~ N(0,1/2).\n",
    "      (ii) G(t) = ( sum_i B_i * phi_i(t) ) / 2.\n",
    "      (iii) Omega^o(t) = G(t)G(t)^T, then soft-threshold off-diagonals.\n",
    "      (iv) Add log10(p)/4 to the diagonal for positive definiteness.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Generate lower-triangular matrices B1,B2,B3,B4\n",
    "    #    with elements from Normal(0, 1/2).\n",
    "    B_list = [np.tril(np.random.normal(0, np.sqrt(0.5), (p, p))) \n",
    "              for _ in range(4)]\n",
    "\n",
    "    # 2. Create time grid from 0 to 1\n",
    "    t_values = np.linspace(0, 1, N)\n",
    "    Omega_list = []\n",
    "\n",
    "    for t in t_values:\n",
    "        # Define phi_1(t), phi_2(t), phi_3(t), phi_4(t)\n",
    "        phi = [\n",
    "            np.sin(np.pi * t / 2),\n",
    "            np.cos(np.pi * t / 2),\n",
    "            np.sin(np.pi * t / 4),\n",
    "            np.cos(np.pi * t / 4)\n",
    "        ]\n",
    "\n",
    "        # 3. Compute G(t) = (B1*phi1 + B2*phi2 + B3*phi3 + B4*phi4) / 2\n",
    "        G = sum(B * ph for B, ph in zip(B_list, phi)) / 2\n",
    "\n",
    "        # Omega^o(t) = G(t)*G(t)^T\n",
    "        Omega_o = G @ G.T  # automatically symmetric\n",
    "\n",
    "        # 4. Soft-threshold off-diagonal elements\n",
    "        #    off_diag = sign(x) * max(|x|-lambda, 0)\n",
    "        off_diag = Omega_o - np.diag(np.diag(Omega_o))  # zero out diag\n",
    "        sign_off = np.sign(off_diag)\n",
    "        mag_off = np.abs(off_diag)\n",
    "        off_diag_thresh = sign_off * np.maximum(mag_off - soft_lambda, 0.0)\n",
    "\n",
    "        # Put thresholded off-diagonals back, keep original diagonal\n",
    "        Omega = np.diag(np.diag(Omega_o)) + off_diag_thresh\n",
    "\n",
    "        # 5. Add log10(p)/4 to the diagonal to ensure positivity\n",
    "        diag_adj = np.log10(p)/4 + eps\n",
    "        Omega += np.eye(p) * diag_adj\n",
    "\n",
    "        # 6. Double-check positive definiteness (just in case)\n",
    "        eigvals, _ = eigh(Omega)\n",
    "        if np.any(eigvals <= 0):\n",
    "            # If not PD, shift up by |min_eig| + eps\n",
    "            Omega += np.eye(p) * (abs(np.min(eigvals)) + eps)\n",
    "\n",
    "        Omega_list.append(Omega)\n",
    "\n",
    "    return Omega_list\n",
    "\n",
    "def generate_synthetic_data(p=10, N=50):\n",
    "    \"\"\"\n",
    "    Generate a time-series dataset X(t) by sampling from\n",
    "    N(0, Sigma(t)), where Sigma(t) = Omega(t)^{-1}.\n",
    "    The code returns:\n",
    "      dataset: shape (N, p)\n",
    "      Omega_list: list of precision matrices for each time grid\n",
    "    \"\"\"\n",
    "    Omega_list = generate_time_varying_precision(p, N)\n",
    "\n",
    "    dataset = np.zeros((N, p))\n",
    "    for i, Omega in enumerate(Omega_list):\n",
    "        # Invert each Omega(t) to get Sigma(t)\n",
    "        try:\n",
    "            Sigma = np.linalg.inv(Omega)\n",
    "        except np.linalg.LinAlgError:\n",
    "            Sigma = np.linalg.pinv(Omega)\n",
    "\n",
    "        # Draw a single sample from N(0, Sigma)\n",
    "        dataset[i] = multivariate_normal.rvs(\n",
    "            mean=np.zeros(p),\n",
    "            cov=Sigma,\n",
    "            size=1\n",
    "        )\n",
    "\n",
    "    # Center the entire dataset\n",
    "    dataset -= dataset.mean(axis=0)\n",
    "    return dataset, Omega_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blockwise ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def admm_loggle(\n",
    "    S_dict,            # dict of empirical covariances: i -> \\hat{Sigma}(t_i), each p x p\n",
    "    time_indices,      # list or set of time indices i in Nk,d\n",
    "    lam,               # penalty weight (omega in your text)\n",
    "    rho,               # ADMM parameter (> 0), recommended ~ lam\n",
    "    alpha=1.5,         # over-relaxation parameter\n",
    "    abs_tol=1e-5,      # absolute tolerance for stopping\n",
    "    rel_tol=1e-3,      # relative tolerance for stopping\n",
    "    max_iter=1000,     # maximum ADMM iterations\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Solve the local group-lasso problem via ADMM for the set of time points in `time_indices`,\n",
    "    i.e. Nk,d = { i : |t_i - t_k| <= d }.\n",
    "\n",
    "    The objective is\n",
    "       min_{Omega,Z} sum_{i in Nk,d} [ tr(Omega(t_i) * S(t_i)) - log det(Omega(t_i)) ]\n",
    "                     + lam * sum_{u!=v} sqrt( sum_{i in Nk,d} Z_uv(t_i)^2 ),\n",
    "    subject to Omega(t_i) - Z(t_i) = 0, Omega(t_i) >> 0, i in Nk,d.\n",
    "\n",
    "    Returns dictionaries:\n",
    "       Omega_dict, Z_dict, U_dict\n",
    "    containing the final ADMM iterates for each time index.\n",
    "\n",
    "    References:\n",
    "    - The \"loggle\" step (i)-(iii) from your question.\n",
    "    - Over-relaxation from Boyd et al. (2011) \"Distributed Optimization and Statistical Learning via ADMM\", Sec. 3.4.3.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1) Initialization ----------------------------------\n",
    "    # We'll store the ADMM iterates in dictionaries keyed by i in time_indices.\n",
    "    # For convenience, let p be the dimension from the shape of any S(t_i).\n",
    "    i0 = time_indices[0]\n",
    "    p = S_dict[i0].shape[0]\n",
    "\n",
    "    # Initialize Z^(0) and U^(0) to zero\n",
    "    Z_dict = {i: np.zeros((p,p), dtype=float) for i in time_indices}\n",
    "    U_dict = {i: np.zeros((p,p), dtype=float) for i in time_indices}\n",
    "    Omega_dict = {i: np.zeros((p,p), dtype=float) for i in time_indices}  # will be updated\n",
    "\n",
    "    # For faster updates, pre-compute identity\n",
    "    I_p = np.eye(p)\n",
    "\n",
    "    # ---- 2) ADMM loop ----------------------------------------\n",
    "    for s in range(max_iter):\n",
    "\n",
    "        # ---- (i) Update Omega^(s)(t_i) for each i in Nk,d ----\n",
    "        for i in time_indices:\n",
    "            # M = \\hat{Sigma}(t_i) - rho ( Z^{s-1}(t_i) - U^{s-1}(t_i) )\n",
    "            M = S_dict[i] - rho*(Z_dict[i] - U_dict[i])\n",
    "\n",
    "            # Eigen-decompose M = Q * Lambda * Q^T\n",
    "            # Then solve for Omega = Q * diag(omega_j) * Q^T\n",
    "            # where omega_j = (-lambda_j + sqrt(lambda_j^2 + 4 rho)) / (2 rho).\n",
    "            eigvals, Q = np.linalg.eigh(M)\n",
    "            # Build the diagonal of solutions\n",
    "            new_eigs = np.array([\n",
    "                (-lmbd + np.sqrt(lmbd**2 + 4.0*rho)) / (2.0*rho)\n",
    "                for lmbd in eigvals\n",
    "            ])\n",
    "            Omega_dict[i] = (Q * new_eigs) @ Q.T  # Q diag(new_eigs) Q^T\n",
    "\n",
    "        # ---- Over-relaxation: O_bar = alpha*Omega + (1-alpha)*Z ----\n",
    "        O_bar_dict = {}\n",
    "        for i in time_indices:\n",
    "            O_bar_dict[i] = alpha * Omega_dict[i] + (1.0 - alpha) * Z_dict[i]\n",
    "\n",
    "        # ---- (ii) Update Z^(s)(t_i) for each i in Nk,d --------\n",
    "        #\n",
    "        #  - Diagonal: Z_{uu}(t_i) = O_bar_{uu}(t_i) + U_{uu}(t_i).\n",
    "        #  - Off-diagonal: group-lasso across i in Nk,d for each (u,v).\n",
    "        #    L_{uv} = sqrt( sum_{i} (O_bar_{uv}(t_i) + U_{uv}(t_i))^2 ).\n",
    "        #    Then Z_{uv}(t_i) = max(1 - lam/(rho * L_{uv}), 0) * (O_bar_{uv}(t_i) + U_{uv}(t_i)).\n",
    "        #\n",
    "        # We do it in a vectorized way per (u,v). The group-lasso shrinks each (u,v) across all i simultaneously.\n",
    "        Z_old = {i: Z_dict[i].copy() for i in time_indices}  # for dual residual\n",
    "\n",
    "        # For each pair (u,v), gather the set of \"O_bar_{uv} + U_{uv}\" across i\n",
    "        # We can do a double loop over u,v or a triple nested loop. \n",
    "        # For large p, you might want to vectorize carefully, but here we keep it more explicit.\n",
    "        for u in range(p):\n",
    "            for v in range(p):\n",
    "                # Diagonal case: no shrink\n",
    "                if u == v:\n",
    "                    for i in time_indices:\n",
    "                        Z_dict[i][u,v] = O_bar_dict[i][u,v] + U_dict[i][u,v]\n",
    "                # Off-diagonal: group-lasso\n",
    "                else:\n",
    "                    # Collect the entire vector [ O_bar_{uv}(ti)+U_{uv}(ti) for i in Nk,d ]\n",
    "                    big_vec = np.array([O_bar_dict[i][u,v] + U_dict[i][u,v] for i in time_indices])\n",
    "                    norm_val = np.linalg.norm(big_vec, 2)  # L_{uv}\n",
    "\n",
    "                    if norm_val == 0.0:\n",
    "                        shrink = 0.0\n",
    "                    else:\n",
    "                        shrink = max(0.0, 1.0 - lam/(rho * norm_val))\n",
    "\n",
    "                    # Update each Z_{uv}(t_i)\n",
    "                    for idx_i, i in enumerate(time_indices):\n",
    "                        Z_dict[i][u,v] = shrink * big_vec[idx_i]\n",
    "\n",
    "        # ---- (iii) Update U^(s)(t_i) for each i in Nk,d -------\n",
    "        for i in time_indices:\n",
    "            # Standard ADMM dual update with *no* over-relaxation here\n",
    "            U_dict[i] = U_dict[i] + (Omega_dict[i] - Z_dict[i])\n",
    "\n",
    "        # ---- 3) Check stopping criteria (primal and dual residuals) ---\n",
    "        # primal residual r^s = Omega^s - Z^s, dual residual d^s = Z^s - Z^(s-1)\n",
    "        r_norm_sq = 0.0\n",
    "        d_norm_sq = 0.0\n",
    "        Om_norm_sq = 0.0\n",
    "        Z_norm_sq = 0.0\n",
    "        U_norm_sq = 0.0\n",
    "\n",
    "        for i in time_indices:\n",
    "            r_ij = Omega_dict[i] - Z_dict[i]\n",
    "            d_ij = Z_dict[i] - Z_old[i]\n",
    "\n",
    "            r_norm_sq += np.sum(r_ij**2)\n",
    "            d_norm_sq += np.sum(d_ij**2)\n",
    "\n",
    "            Om_norm_sq += np.sum(Omega_dict[i]**2)\n",
    "            Z_norm_sq += np.sum(Z_dict[i]**2)\n",
    "            U_norm_sq += np.sum(U_dict[i]**2)\n",
    "\n",
    "        r_norm = np.sqrt(r_norm_sq)\n",
    "        d_norm = np.sqrt(d_norm_sq)\n",
    "\n",
    "        # primal feasibility tolerance\n",
    "        eps_pri = abs_tol*np.sqrt(p*p*len(time_indices)) + \\\n",
    "                  rel_tol*max(np.sqrt(Om_norm_sq), np.sqrt(Z_norm_sq))\n",
    "\n",
    "        # dual feasibility tolerance\n",
    "        eps_dual = abs_tol*np.sqrt(p*p*len(time_indices)) + \\\n",
    "                   rel_tol*np.sqrt(U_norm_sq)\n",
    "\n",
    "        if verbose and (s % 50 == 0):\n",
    "            print(f\"ADMM iter {s:4d}: r_norm={r_norm:.3e}, d_norm={d_norm:.3e},\"\n",
    "                  f\" eps_pri={eps_pri:.3e}, eps_dual={eps_dual:.3e}\")\n",
    "\n",
    "        if (r_norm <= eps_pri) and (d_norm <= eps_dual):\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {s}\")\n",
    "            break\n",
    "\n",
    "    return Omega_dict, Z_dict, U_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate covariance at all times\n",
    "\n",
    "def gaussian_kernel(u, normalize=True):\n",
    "    \"\"\"\n",
    "    Standard Gaussian kernel K(u) = exp(-u^2/2) / sqrt(2*pi), if normalize=True.\n",
    "    If normalize=False, we drop 1/sqrt(2*pi) and just use exp(-u^2/2).\n",
    "    \"\"\"\n",
    "    c = 1.0 / np.sqrt(2.0 * np.pi) if normalize else 1.0\n",
    "    return c * np.exp(-0.5 * u**2)\n",
    "\n",
    "def kernel_smoothed_cov(x_data, times, t_target, h, kernel_fun=gaussian_kernel):\n",
    "    \"\"\"\n",
    "    Compute the kernel-smoothed covariance at a single target time t_target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : numpy array of shape (N, p)\n",
    "        Observations x_j in R^p. The j-th row is x_j^T.\n",
    "    times : numpy array of shape (N,)\n",
    "        Observation times corresponding to each x_j.\n",
    "    t_target : float\n",
    "        The target time at which we want \\hat{Sigma}(t_target).\n",
    "    h : float\n",
    "        Bandwidth for the kernel.\n",
    "    kernel_fun : callable\n",
    "        A kernel function K(u), e.g. Gaussian.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cov_matrix : (p, p) numpy array\n",
    "        The kernel-smoothed covariance matrix at time t_target.\n",
    "    \"\"\"\n",
    "    N, p = x_data.shape\n",
    "\n",
    "    # Compute all weights w_j = K((t_j - t_target)/h).\n",
    "    diffs = (times - t_target) / h\n",
    "    weights = np.array([kernel_fun(d) for d in diffs])\n",
    "\n",
    "    # Normalize weights so that they sum to 1.\n",
    "    w_sum = weights.sum()\n",
    "    if w_sum <= 1e-14:\n",
    "        # Edge case: if no points get any weight, return something safe (e.g. zero or unweighted).\n",
    "        # Or you could just return the empirical covariance unweighted.\n",
    "        return np.cov(x_data.T, bias=True)\n",
    "\n",
    "    w_normalized = weights / w_sum\n",
    "\n",
    "    # Accumulate weighted outer products x_j x_j^T\n",
    "    cov_matrix = np.zeros((p,p), dtype=float)\n",
    "    for j in range(N):\n",
    "        x_j = x_data[j]  # shape (p,)\n",
    "        cov_matrix += w_normalized[j] * np.outer(x_j, x_j)\n",
    "\n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_smoothed_covs_at_observed_times(x_data, times, h, kernel_fun=gaussian_kernel):\n",
    "    \"\"\"\n",
    "    Compute kernel-smoothed covariance at each observed time in 'times'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cov_dict : dict\n",
    "        Keys: i = 0, 1, ..., N-1\n",
    "        Values: (p,p) numpy arrays, \\hat{Sigma}(t_i)\n",
    "    \"\"\"\n",
    "    N = len(times)\n",
    "    cov_dict = {}\n",
    "    for i, t_i in enumerate(times):\n",
    "        cov_dict[i] = kernel_smoothed_cov(x_data, times, t_i, h, kernel_fun)\n",
    "    return cov_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
