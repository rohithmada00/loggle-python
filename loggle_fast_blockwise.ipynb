{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rohit\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\rohit\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.linalg import eigh, inv, det\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_varying_precision(p=10, N=50, eps=1e-6, soft_lambda=0.14):\n",
    "    \"\"\"\n",
    "    Generate a list of time-varying precision matrices Omega(t),\n",
    "    following the four steps (i)-(iv) described:\n",
    "\n",
    "      (i)  B_i are lower-triangular with entries ~ N(0,1/2).\n",
    "      (ii) G(t) = ( sum_i B_i * phi_i(t) ) / 2.\n",
    "      (iii) Omega^o(t) = G(t)G(t)^T, then soft-threshold off-diagonals.\n",
    "      (iv) Add log10(p)/4 to the diagonal for positive definiteness.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Generate lower-triangular matrices B1,B2,B3,B4\n",
    "    #    with elements from Normal(0, 1/2).\n",
    "    B_list = [np.tril(np.random.normal(0, np.sqrt(0.5), (p, p))) \n",
    "              for _ in range(4)]\n",
    "\n",
    "    # 2. Create time grid from 0 to 1\n",
    "    t_values = np.linspace(0, 1, N)\n",
    "    Omega_list = []\n",
    "\n",
    "    for t in t_values:\n",
    "        # Define phi_1(t), phi_2(t), phi_3(t), phi_4(t)\n",
    "        phi = [\n",
    "            np.sin(np.pi * t / 2),\n",
    "            np.cos(np.pi * t / 2),\n",
    "            np.sin(np.pi * t / 4),\n",
    "            np.cos(np.pi * t / 4)\n",
    "        ]\n",
    "\n",
    "        # 3. Compute G(t) = (B1*phi1 + B2*phi2 + B3*phi3 + B4*phi4) / 2\n",
    "        G = sum(B * ph for B, ph in zip(B_list, phi)) / 2\n",
    "\n",
    "        # Omega^o(t) = G(t)*G(t)^T\n",
    "        Omega_o = G @ G.T  # automatically symmetric\n",
    "\n",
    "        # 4. Soft-threshold off-diagonal elements\n",
    "        #    off_diag = sign(x) * max(|x|-lambda, 0)\n",
    "        off_diag = Omega_o - np.diag(np.diag(Omega_o))  # zero out diag\n",
    "        sign_off = np.sign(off_diag)\n",
    "        mag_off = np.abs(off_diag)\n",
    "        off_diag_thresh = sign_off * np.maximum(mag_off - soft_lambda, 0.0)\n",
    "\n",
    "        # Put thresholded off-diagonals back, keep original diagonal\n",
    "        Omega = np.diag(np.diag(Omega_o)) + off_diag_thresh\n",
    "\n",
    "        # 5. Add log10(p)/4 to the diagonal to ensure positivity\n",
    "        diag_adj = np.log10(p)/4 + eps\n",
    "        Omega += np.eye(p) * diag_adj\n",
    "\n",
    "        # 6. Double-check positive definiteness (just in case)\n",
    "        eigvals, _ = eigh(Omega)\n",
    "        if np.any(eigvals <= 0):\n",
    "            # If not PD, shift up by |min_eig| + eps\n",
    "            Omega += np.eye(p) * (abs(np.min(eigvals)) + eps)\n",
    "\n",
    "        Omega_list.append(Omega)\n",
    "\n",
    "    return Omega_list\n",
    "\n",
    "def generate_synthetic_data(p=10, N=50):\n",
    "    \"\"\"\n",
    "    Generate a time-series dataset X(t) by sampling from\n",
    "    N(0, Sigma(t)), where Sigma(t) = Omega(t)^{-1}.\n",
    "    The code returns:\n",
    "      dataset: shape (N, p)\n",
    "      Omega_list: list of precision matrices for each time grid\n",
    "    \"\"\"\n",
    "    Omega_list = generate_time_varying_precision(p, N)\n",
    "\n",
    "    dataset = np.zeros((N, p))\n",
    "    for i, Omega in enumerate(Omega_list):\n",
    "        # Invert each Omega(t) to get Sigma(t)\n",
    "        try:\n",
    "            Sigma = np.linalg.inv(Omega)\n",
    "        except np.linalg.LinAlgError:\n",
    "            Sigma = np.linalg.pinv(Omega)\n",
    "\n",
    "        # Draw a single sample from N(0, Sigma)\n",
    "        dataset[i] = multivariate_normal.rvs(\n",
    "            mean=np.zeros(p),\n",
    "            cov=Sigma,\n",
    "            size=1\n",
    "        )\n",
    "\n",
    "    # Center the entire dataset\n",
    "    dataset -= dataset.mean(axis=0)\n",
    "    return dataset, Omega_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blockwise ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def admm_loggle(\n",
    "    S_dict,            # dict of empirical covariances: i -> \\hat{Sigma}(t_i), each p x p\n",
    "    time_indices,      # list or set of time indices i in Nk,d\n",
    "    lam,               # penalty weight (omega in your text)\n",
    "    rho,               # ADMM parameter (> 0), recommended ~ lam\n",
    "    alpha=1.5,         # over-relaxation parameter\n",
    "    abs_tol=1e-5,      # absolute tolerance for stopping\n",
    "    rel_tol=1e-3,      # relative tolerance for stopping\n",
    "    max_iter=1000,     # maximum ADMM iterations\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Solve the local group-lasso problem via ADMM for the set of time points in `time_indices`,\n",
    "    i.e. Nk,d = { i : |t_i - t_k| <= d }.\n",
    "\n",
    "    The objective is\n",
    "       min_{Omega,Z} sum_{i in Nk,d} [ tr(Omega(t_i) * S(t_i)) - log det(Omega(t_i)) ]\n",
    "                     + lam * sum_{u!=v} sqrt( sum_{i in Nk,d} Z_uv(t_i)^2 ),\n",
    "    subject to Omega(t_i) - Z(t_i) = 0, Omega(t_i) >> 0, i in Nk,d.\n",
    "\n",
    "    Returns dictionaries:\n",
    "       Omega_dict, Z_dict, U_dict\n",
    "    containing the final ADMM iterates for each time index.\n",
    "\n",
    "    References:\n",
    "    - The \"loggle\" step (i)-(iii) from your question.\n",
    "    - Over-relaxation from Boyd et al. (2011) \"Distributed Optimization and Statistical Learning via ADMM\", Sec. 3.4.3.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1) Initialization ----------------------------------\n",
    "    # We'll store the ADMM iterates in dictionaries keyed by i in time_indices.\n",
    "    # For convenience, let p be the dimension from the shape of any S(t_i).\n",
    "    i0 = time_indices[0]\n",
    "    p = S_dict[i0].shape[0]\n",
    "\n",
    "    # Initialize Z^(0) and U^(0) to zero\n",
    "    Z_dict = {i: np.zeros((p,p), dtype=float) for i in time_indices}\n",
    "    U_dict = {i: np.zeros((p,p), dtype=float) for i in time_indices}\n",
    "    Omega_dict = {i: np.zeros((p,p), dtype=float) for i in time_indices}  # will be updated\n",
    "\n",
    "    # For faster updates, pre-compute identity\n",
    "    I_p = np.eye(p)\n",
    "\n",
    "    # ---- 2) ADMM loop ----------------------------------------\n",
    "    for s in range(max_iter):\n",
    "\n",
    "        # ---- (i) Update Omega^(s)(t_i) for each i in Nk,d ----\n",
    "        for i in time_indices:\n",
    "            # M = \\hat{Sigma}(t_i) - rho ( Z^{s-1}(t_i) - U^{s-1}(t_i) )\n",
    "            M = S_dict[i] - rho*(Z_dict[i] - U_dict[i])\n",
    "\n",
    "            # Eigen-decompose M = Q * Lambda * Q^T\n",
    "            # Then solve for Omega = Q * diag(omega_j) * Q^T\n",
    "            # where omega_j = (-lambda_j + sqrt(lambda_j^2 + 4 rho)) / (2 rho).\n",
    "            eigvals, Q = np.linalg.eigh(M)\n",
    "            # Build the diagonal of solutions\n",
    "            new_eigs = np.array([\n",
    "                (-lmbd + np.sqrt(lmbd**2 + 4.0*rho)) / (2.0*rho)\n",
    "                for lmbd in eigvals\n",
    "            ])\n",
    "            Omega_dict[i] = (Q * new_eigs) @ Q.T  # Q diag(new_eigs) Q^T\n",
    "\n",
    "        # ---- Over-relaxation: O_bar = alpha*Omega + (1-alpha)*Z ----\n",
    "        O_bar_dict = {}\n",
    "        for i in time_indices:\n",
    "            O_bar_dict[i] = alpha * Omega_dict[i] + (1.0 - alpha) * Z_dict[i]\n",
    "\n",
    "        # ---- (ii) Update Z^(s)(t_i) for each i in Nk,d --------\n",
    "        #\n",
    "        #  - Diagonal: Z_{uu}(t_i) = O_bar_{uu}(t_i) + U_{uu}(t_i).\n",
    "        #  - Off-diagonal: group-lasso across i in Nk,d for each (u,v).\n",
    "        #    L_{uv} = sqrt( sum_{i} (O_bar_{uv}(t_i) + U_{uv}(t_i))^2 ).\n",
    "        #    Then Z_{uv}(t_i) = max(1 - lam/(rho * L_{uv}), 0) * (O_bar_{uv}(t_i) + U_{uv}(t_i)).\n",
    "        #\n",
    "        # We do it in a vectorized way per (u,v). The group-lasso shrinks each (u,v) across all i simultaneously.\n",
    "        Z_old = {i: Z_dict[i].copy() for i in time_indices}  # for dual residual\n",
    "\n",
    "        # For each pair (u,v), gather the set of \"O_bar_{uv} + U_{uv}\" across i\n",
    "        # We can do a double loop over u,v or a triple nested loop. \n",
    "        # For large p, you might want to vectorize carefully, but here we keep it more explicit.\n",
    "        for u in range(p):\n",
    "            for v in range(p):\n",
    "                # Diagonal case: no shrink\n",
    "                if u == v:\n",
    "                    for i in time_indices:\n",
    "                        Z_dict[i][u,v] = O_bar_dict[i][u,v] + U_dict[i][u,v]\n",
    "                # Off-diagonal: group-lasso\n",
    "                else:\n",
    "                    # Collect the entire vector [ O_bar_{uv}(ti)+U_{uv}(ti) for i in Nk,d ]\n",
    "                    big_vec = np.array([O_bar_dict[i][u,v] + U_dict[i][u,v] for i in time_indices])\n",
    "                    norm_val = np.linalg.norm(big_vec, 2)  # L_{uv}\n",
    "\n",
    "                    if norm_val == 0.0:\n",
    "                        shrink = 0.0\n",
    "                    else:\n",
    "                        shrink = max(0.0, 1.0 - lam/(rho * norm_val))\n",
    "\n",
    "                    # Update each Z_{uv}(t_i)\n",
    "                    for idx_i, i in enumerate(time_indices):\n",
    "                        Z_dict[i][u,v] = shrink * big_vec[idx_i]\n",
    "\n",
    "        # ---- (iii) Update U^(s)(t_i) for each i in Nk,d -------\n",
    "        for i in time_indices:\n",
    "            # Standard ADMM dual update with *no* over-relaxation here\n",
    "            U_dict[i] = U_dict[i] + (Omega_dict[i] - Z_dict[i])\n",
    "\n",
    "        # ---- 3) Check stopping criteria (primal and dual residuals) ---\n",
    "        # primal residual r^s = Omega^s - Z^s, dual residual d^s = Z^s - Z^(s-1)\n",
    "        r_norm_sq = 0.0\n",
    "        d_norm_sq = 0.0\n",
    "        Om_norm_sq = 0.0\n",
    "        Z_norm_sq = 0.0\n",
    "        U_norm_sq = 0.0\n",
    "\n",
    "        for i in time_indices:\n",
    "            r_ij = Omega_dict[i] - Z_dict[i]\n",
    "            d_ij = Z_dict[i] - Z_old[i]\n",
    "\n",
    "            r_norm_sq += np.sum(r_ij**2)\n",
    "            d_norm_sq += np.sum(d_ij**2)\n",
    "\n",
    "            Om_norm_sq += np.sum(Omega_dict[i]**2)\n",
    "            Z_norm_sq += np.sum(Z_dict[i]**2)\n",
    "            U_norm_sq += np.sum(U_dict[i]**2)\n",
    "\n",
    "        r_norm = np.sqrt(r_norm_sq)\n",
    "        d_norm = np.sqrt(d_norm_sq)\n",
    "\n",
    "        # primal feasibility tolerance\n",
    "        eps_pri = abs_tol*np.sqrt(p*p*len(time_indices)) + \\\n",
    "                  rel_tol*max(np.sqrt(Om_norm_sq), np.sqrt(Z_norm_sq))\n",
    "\n",
    "        # dual feasibility tolerance\n",
    "        eps_dual = abs_tol*np.sqrt(p*p*len(time_indices)) + \\\n",
    "                   rel_tol*np.sqrt(U_norm_sq)\n",
    "\n",
    "        if verbose and (s % 50 == 0):\n",
    "            print(f\"ADMM iter {s:4d}: r_norm={r_norm:.3e}, d_norm={d_norm:.3e},\"\n",
    "                  f\" eps_pri={eps_pri:.3e}, eps_dual={eps_dual:.3e}\")\n",
    "\n",
    "        if (r_norm <= eps_pri) and (d_norm <= eps_dual):\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {s}\")\n",
    "            break\n",
    "\n",
    "    return Omega_dict, Z_dict, U_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(u, normalize=True):\n",
    "    \"\"\"\n",
    "    Standard Gaussian kernel K(u) = exp(-u^2/2) / sqrt(2*pi), if normalize=True.\n",
    "    If normalize=False, we drop 1/sqrt(2*pi) and just use exp(-u^2/2).\n",
    "    \"\"\"\n",
    "    c = 1.0 / np.sqrt(2.0 * np.pi) if normalize else 1.0\n",
    "    return c * np.exp(-0.5 * u**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_smoothed_cov(x_data, times, t_target, h, kernel_fun=gaussian_kernel):\n",
    "    \"\"\"\n",
    "    Compute the kernel-smoothed covariance at a single target time t_target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : numpy array of shape (N, p)\n",
    "        Observations x_j in R^p. The j-th row is x_j^T.\n",
    "    times : numpy array of shape (N,)\n",
    "        Observation times corresponding to each x_j.\n",
    "    t_target : float\n",
    "        The target time at which we want \\hat{Sigma}(t_target).\n",
    "    h : float\n",
    "        Bandwidth for the kernel.\n",
    "    kernel_fun : callable\n",
    "        A kernel function K(u), e.g. Gaussian.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cov_matrix : (p, p) numpy array\n",
    "        The kernel-smoothed covariance matrix at time t_target.\n",
    "    \"\"\"\n",
    "    N, p = x_data.shape\n",
    "\n",
    "    # Compute all weights w_j = K((t_j - t_target)/h).\n",
    "    diffs = (times - t_target) / h\n",
    "    weights = np.array([kernel_fun(d) for d in diffs])\n",
    "\n",
    "    # Normalize weights so that they sum to 1.\n",
    "    w_sum = weights.sum()\n",
    "    if w_sum <= 1e-14:\n",
    "        # Edge case: if no points get any weight, return something safe (e.g. zero or unweighted).\n",
    "        # Or you could just return the empirical covariance unweighted.\n",
    "        return np.cov(x_data.T, bias=True)\n",
    "\n",
    "    w_normalized = weights / w_sum\n",
    "\n",
    "    # Accumulate weighted outer products x_j x_j^T\n",
    "    cov_matrix = np.zeros((p,p), dtype=float)\n",
    "    for j in range(N):\n",
    "        x_j = x_data[j]  # shape (p,)\n",
    "        cov_matrix += w_normalized[j] * np.outer(x_j, x_j)\n",
    "\n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_smoothed_covs_at_observed_times(x_data, times, h, kernel_fun=gaussian_kernel):\n",
    "    \"\"\"\n",
    "    Compute kernel-smoothed covariance at each observed time in 'times'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cov_dict : dict\n",
    "        Keys: i = 0, 1, ..., N-1\n",
    "        Values: (p,p) numpy arrays, \\hat{Sigma}(t_i)\n",
    "    \"\"\"\n",
    "    N = len(times)\n",
    "    cov_dict = {}\n",
    "    for i, t_i in enumerate(times):\n",
    "        cov_dict[i] = kernel_smoothed_cov(x_data, times, t_i, h, kernel_fun)\n",
    "    return cov_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_set(Omega, threshold=1e-4):\n",
    "    \"\"\"\n",
    "    Return the set of edges (u,v) with u < v where |Omega[u,v]| > threshold.\n",
    "    Diagonal entries are excluded by construction.\n",
    "    \"\"\"\n",
    "    p = Omega.shape[0]\n",
    "    edges = set()\n",
    "    for u in range(p):\n",
    "        for v in range(u+1, p):\n",
    "            if abs(Omega[u, v]) > threshold:\n",
    "                edges.add((u, v))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fdr_power_f1(true_edge_sets, est_edge_sets):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      true_edge_sets: list of sets S_k\n",
    "      est_edge_sets : list of sets S^_k\n",
    "    Returns: (FDR, power, F1)\n",
    "    \"\"\"\n",
    "    K = len(true_edge_sets)\n",
    "    if K == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    sum_tp_over_est = 0.0  # sum of (true positives / #est_edges) across k\n",
    "    sum_tp_over_true = 0.0 # sum of (true positives / #true_edges) across k\n",
    "\n",
    "    for k in range(K):\n",
    "        S_k = true_edge_sets[k]\n",
    "        Shat_k = est_edge_sets[k]\n",
    "        if len(Shat_k) == 0:\n",
    "            # If no edges estimated, fraction is 0\n",
    "            tp_over_est = 0\n",
    "        else:\n",
    "            tp_over_est = len(S_k.intersection(Shat_k)) / len(Shat_k)\n",
    "\n",
    "        if len(S_k) == 0:\n",
    "            tp_over_true = 0\n",
    "        else:\n",
    "            tp_over_true = len(S_k.intersection(Shat_k)) / len(S_k)\n",
    "\n",
    "        sum_tp_over_est += tp_over_est\n",
    "        sum_tp_over_true += tp_over_true\n",
    "\n",
    "    avg_tp_over_est = sum_tp_over_est / K  # (1 - FDR)\n",
    "    avg_tp_over_true = sum_tp_over_true / K # power\n",
    "\n",
    "    FDR = 1 - avg_tp_over_est\n",
    "    power = avg_tp_over_true\n",
    "    # F1 = 2 * (1-FDR)*power / ((1-FDR)+power)\n",
    "    denom = (1 - FDR) + power\n",
    "    if denom < 1e-15:\n",
    "        F1 = 0.0\n",
    "    else:\n",
    "        F1 = 2.0 * (1 - FDR) * power / denom\n",
    "\n",
    "    return FDR, power, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(true_prec_list, est_prec_list):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      true_prec_list: length-K list of \"true\" precision matrices\n",
    "      est_prec_list : length-K list of \"estimated\" precision matrices\n",
    "    Returns:\n",
    "      scalar float for average KL divergence\n",
    "    \"\"\"\n",
    "    K = len(true_prec_list)\n",
    "    if K == 0:\n",
    "        return 0.0\n",
    "\n",
    "    p = true_prec_list[0].shape[0]\n",
    "    kl_vals = []\n",
    "    for k in range(K):\n",
    "        O_true = true_prec_list[k]\n",
    "        O_est  = est_prec_list[k]\n",
    "\n",
    "        O_true_inv = inv(O_true)  # or store if you already have\n",
    "        prod = O_est @ O_true_inv\n",
    "\n",
    "        # KL part: tr( O_est * O_true^-1 ) - log det( O_est*O_true^-1 ) - p\n",
    "        tr_part = np.trace(prod)\n",
    "        # numeric safeguard in case det is near 0\n",
    "        logdet_part = math.log(np.linalg.det(prod) + 1e-15)\n",
    "        kl_val = tr_part - logdet_part - p\n",
    "        kl_vals.append(kl_val)\n",
    "\n",
    "    return np.mean(kl_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Evaluation Results ==========\n",
      "FDR   = 0.3386\n",
      "Power = 0.8766\n",
      "F1    = 0.7539\n",
      "KL    = 6.7728\n"
     ]
    }
   ],
   "source": [
    "p = 10\n",
    "N = 50\n",
    "dataset, Omega_list_true = generate_synthetic_data(p=p, N=N)\n",
    "#  --> dataset: shape (N, p)\n",
    "#  --> Omega_list_true: length-N list of \"true\" Omega(t_i)\n",
    "\n",
    "# For convenience, let's define time points same as generate_time_varying_precision:\n",
    "times = np.linspace(0, 1, N)\n",
    "\n",
    "#  We'll get a \"local\" estimate at each i in 0..N-1 by running ADMM\n",
    "#            on the neighborhood Nk,d = { j : |times[j]-times[i]| <= d }.\n",
    "d = 0.1 # Example neighborhood half-width\n",
    "lam = 0.1\n",
    "rho = 0.1\n",
    "\n",
    "# We'll store the final ADMM-estimated Omega(t_i) in a list\n",
    "Omega_list_est = [None]*N\n",
    "\n",
    "# First, compute kernel-smoothed covariance at each time for the entire dataset\n",
    "# so that we have \\hat{Sigma}(t_j) for j in 0..N-1\n",
    "h = 0.08\n",
    "S_dict_global = kernel_smoothed_covs_at_observed_times(dataset, times, h)\n",
    "\n",
    "# Now, for each i, build the sub-dict S_dict_i from the relevant Nk,d\n",
    "for i in range(N):\n",
    "    # Find the local neighborhood\n",
    "    Nk_d = [ j for j in range(N) if abs(times[j] - times[i]) <= d ]\n",
    "\n",
    "    # Build the sub-dict S_dict for that neighborhood\n",
    "    S_dict_local = { j: S_dict_global[j] for j in Nk_d }\n",
    "\n",
    "    # Solve the local problem\n",
    "    Omega_dict_i, Z_dict_i, U_dict_i = admm_loggle(\n",
    "        S_dict=S_dict_local,\n",
    "        time_indices=Nk_d,\n",
    "        lam=lam,\n",
    "        rho=rho,\n",
    "        alpha=1.5,\n",
    "        abs_tol=1e-5,\n",
    "        rel_tol=1e-3,\n",
    "        max_iter=300,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # After ADMM converges, we want the estimate of Omega(t_i).\n",
    "    # It's stored in Omega_dict_i[i], because i is in Nk_d.\n",
    "    # (If i is not in Nk_d for some reason, we can't get it, but typically\n",
    "    #  i is definitely in its own neighborhood.)\n",
    "    Omega_list_est[i] = Omega_dict_i[i]\n",
    "\n",
    "# -- Step 3: Evaluate performance across all i in 0..N-1\n",
    "true_edge_sets = []\n",
    "est_edge_sets = []\n",
    "for i in range(N):\n",
    "    # Extract edges from the \"true\" Omega and from the \"estimated\" one\n",
    "    true_edges = get_edge_set(Omega_list_true[i], threshold=1e-3)\n",
    "    est_edges  = get_edge_set(Omega_list_est[i], threshold=1e-2)\n",
    "\n",
    "    true_edge_sets.append(true_edges)\n",
    "    est_edge_sets.append(est_edges)\n",
    "\n",
    "# Compute FDR, power, F1\n",
    "FDR, power, F1 = compute_fdr_power_f1(true_edge_sets, est_edge_sets)\n",
    "\n",
    "# Compute KL\n",
    "KL = compute_kl_divergence(Omega_list_true, Omega_list_est)\n",
    "\n",
    "# Print results\n",
    "print(\"========== Evaluation Results ==========\")\n",
    "print(f\"FDR   = {FDR:.4f}\")\n",
    "print(f\"Power = {power:.4f}\")\n",
    "print(f\"F1    = {F1:.4f}\")\n",
    "print(f\"KL    = {KL:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
